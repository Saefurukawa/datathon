{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcategory_encoders\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mce\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(\"./datathon_train.csv\")\n",
    "\n",
    "def get_time(string):\n",
    "  return int(string[:2])\n",
    "dataset['DEP_TIME'] = dataset['DEP_TIME_BLK'].apply(get_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clean = dataset.drop(['Id', 'ORIGIN_CITY_NAME', 'DEST_CITY_NAME', 'DEP_TIME_BLK','DEST'], axis = 1)\n",
    "dataset_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'X_df' is your dataset and 'Departing Airport Name' is a categorical column\n",
    "categorical_columns = ['PREVIOUS_AIRPORT', 'CARRIER_NAME', 'DEPARTING_AIRPORT']\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(dataset_clean[categorical_columns])\n",
    "\n",
    "# Drop the original categorical columns from the original DataFrame\n",
    "dataset_clean = dataset_clean.drop(categorical_columns, axis=1)\n",
    "\n",
    "# Convert the one-hot encoded NumPy array back to a DataFrame with feature names\n",
    "encoded_feature_names = []\n",
    "for category, column in zip(encoder.categories_, categorical_columns):\n",
    "    encoded_feature_names.extend([f\"{column}_{cat}\" for cat in category])\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_feature_names)\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original dataset\n",
    "X_df = pd.concat([dataset_clean, X_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_df['IS_DELAYED'].to_numpy()\n",
    "New_X_df = X_df.drop(['DEP_DELAY_NEW', 'IS_DELAYED'], axis = 1)\n",
    "X = New_X_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 60% training, 20% validation, and 20% testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Split the remaining data into 50% validation and 50% testing\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your training data and transform it\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "# Shuffle the training data\n",
    "X_train_normalized, y_train = shuffle(X_train_normalized, y_train, random_state=42)\n",
    "\n",
    "# Shuffle the validation data (if needed)\n",
    "X_val_normalized, y_eval = shuffle(X_val_normalized, y_val, random_state=42)\n",
    "\n",
    "# Shuffle the development data\n",
    "X_dev_normalized, y_dev = shuffle(X_test_normalized, y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Dense(50, activation='relu', input_shape=(63,)))\n",
    "\n",
    "model_1.add(Dense(30, activation='relu'))\n",
    "\n",
    "model_1.add(Dense(10, activation='relu'))\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = model_1.fit(X_train, y_train, epochs=10, batch_size=32, vvalidation_data=(X_val, y_val))\n",
    "y_pred_m1 = model_1.predict(X_test)\n",
    "y_pred_m1 =(y_pred_m1 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13 (main, Sep 11 2023, 13:21:10) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "513d186a859c888c0ae87928867d531a5f003b28dc95e180424e2aef27bc2e74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
